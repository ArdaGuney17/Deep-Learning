{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Please follow the steps below to run the code.\n",
        "\n",
        "How to Set Up:\n",
        "\n",
        "1.   Please connect your google drive to the notebook for reaching the folder that has the rquired files.\n",
        "2.   By this link please reach the folder and save it to your google drive. Link to the folder, please put every document inside this to the same folder in your drive: https://drive.google.com/drive/folders/1thP3uznlo-pQkio5ql2pZxBYfM_1iyZE?usp=sharing\n",
        "3.   Connect the folder in the drive to your code by changing the local_model_path parameter in second cell (local_model_path = \"/content/drive/My Drive/DeepLearning\") to the location of the folder in your Google Drive.\n",
        "4.   By this link reach the pickle file that contains the data. Link to the pickle file: https://drive.google.com/file/d/1G3iOzaCgHUVOpjrrea75oMO_-A1H7nyz/view?usp=sharing\n",
        "5.   Upload the pickle file to your notebook.\n",
        "6.   You are redy to run!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eQa4hhcYvb_O"
      },
      "id": "eQa4hhcYvb_O"
    },
    {
      "cell_type": "markdown",
      "id": "15a609e3-5a4f-43be-9e59-767997de7be5",
      "metadata": {
        "id": "15a609e3-5a4f-43be-9e59-767997de7be5"
      },
      "source": [
        "## Deep Learning Project\n",
        "\n",
        "T5 is\n",
        "a text-to-text Transformer (**Text-to-Text Transfer Transformer**\n",
        "(T5)) trained with a similar\n",
        "masked language modeling objective as BERT. In\n",
        "this model, all target tasks are cast as sequenceto-sequence tasks.\n",
        "\n",
        "If you’ve downloaded the pytorch_model.bin file and associated model configuration files (like config.json), you can load the model locally.\n",
        "\n",
        "Ensure the following files are in the same directory:\n",
        "- pytorch_model.bin\n",
        "- config.json\n",
        "- Tokenizer files like tokenizer.json or vocab.txt.\n",
        "\n",
        "Specify the directory when loading the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "au-aHFtuDGM_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au-aHFtuDGM_",
        "outputId": "fadf9d36-a88c-4166-c24d-4dd681c2950b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "24b0f14c-3869-408e-837d-8ace1cd9099f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24b0f14c-3869-408e-837d-8ace1cd9099f",
        "outputId": "f1fcfcc7-8272-4e43-9031-31b96f8fe2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Path to the local directory containing the model files, use \\\\ or /\n",
        "local_model_path = \"/content/drive/My Drive/DeepLearning\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(local_model_path)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eb3e6779-b4ba-4752-9441-694a8ec0e571",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb3e6779-b4ba-4752-9441-694a8ec0e571",
        "outputId": "8c20de57-7af0-4435-9128-c29de260cb6d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: shared.weight, Shape: torch.Size([32128, 768])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Shape: torch.Size([32, 12])\n",
            "Layer: encoder.block.0.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.0.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.0.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.0.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.1.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.1.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.1.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.1.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.1.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.1.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.1.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.1.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.2.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.2.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.2.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.2.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.2.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.2.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.2.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.2.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.3.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.3.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.3.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.3.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.3.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.3.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.3.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.3.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.4.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.4.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.4.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.4.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.4.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.4.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.4.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.4.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.5.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.5.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.5.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.5.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.5.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.5.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.5.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.5.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.6.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.6.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.6.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.6.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.6.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.6.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.6.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.6.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.7.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.7.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.7.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.7.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.7.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.7.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.7.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.7.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.8.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.8.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.8.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.8.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.8.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.8.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.8.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.8.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.9.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.9.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.9.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.9.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.9.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.9.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.9.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.9.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.10.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.10.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.10.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.10.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.10.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.10.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.10.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.10.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.11.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.11.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.11.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.11.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: encoder.block.11.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.block.11.layer.1.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: encoder.block.11.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: encoder.block.11.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: encoder.final_layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Shape: torch.Size([32, 12])\n",
            "Layer: decoder.block.0.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.0.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.0.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.0.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.0.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.0.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.0.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.0.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.0.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.1.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.1.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.1.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.1.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.1.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.1.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.1.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.1.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.1.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.1.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.1.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.1.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.1.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.2.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.2.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.2.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.2.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.2.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.2.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.2.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.2.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.2.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.2.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.2.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.2.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.2.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.3.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.3.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.3.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.3.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.3.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.3.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.3.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.3.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.3.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.3.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.3.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.3.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.3.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.4.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.4.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.4.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.4.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.4.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.4.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.4.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.4.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.4.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.4.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.4.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.4.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.4.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.5.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.5.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.5.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.5.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.5.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.5.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.5.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.5.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.5.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.5.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.5.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.5.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.5.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.6.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.6.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.6.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.6.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.6.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.6.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.6.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.6.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.6.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.6.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.6.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.6.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.6.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.7.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.7.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.7.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.7.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.7.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.7.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.7.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.7.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.7.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.7.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.7.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.7.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.7.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.8.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.8.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.8.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.8.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.8.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.8.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.8.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.8.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.8.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.8.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.8.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.8.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.8.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.9.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.9.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.9.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.9.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.9.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.9.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.9.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.9.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.9.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.9.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.9.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.9.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.9.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.10.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.10.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.10.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.10.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.10.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.10.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.10.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.10.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.10.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.10.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.10.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.10.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.10.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.11.layer.0.SelfAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.11.layer.0.SelfAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.11.layer.0.SelfAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.11.layer.0.SelfAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.11.layer.0.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.11.layer.1.EncDecAttention.q.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.11.layer.1.EncDecAttention.k.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.11.layer.1.EncDecAttention.v.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.11.layer.1.EncDecAttention.o.weight, Shape: torch.Size([768, 768])\n",
            "Layer: decoder.block.11.layer.1.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.block.11.layer.2.DenseReluDense.wi.weight, Shape: torch.Size([3072, 768])\n",
            "Layer: decoder.block.11.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([768, 3072])\n",
            "Layer: decoder.block.11.layer.2.layer_norm.weight, Shape: torch.Size([768])\n",
            "Layer: decoder.final_layer_norm.weight, Shape: torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "# Access the model's parameters\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name}, Shape: {param.shape}\")\n",
        "\n",
        "# Save a specific layer's weights for inspection\n",
        "# specific_layer = dict(model.named_parameters())[\"encoder.embed_tokens.weight\"]\n",
        "# print(specific_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c833e8d-1a76-4fc7-865e-c66f92939a0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c833e8d-1a76-4fc7-865e-c66f92939a0c",
        "outputId": "919b50e1-0e58-4b82-f1c1-643574dc7778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768, 768)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert parameters from a specific layer to a NumPy array\n",
        "specific_layer1 = dict(model.named_parameters())[\"encoder.block.0.layer.0.SelfAttention.q.weight\"]\n",
        "specific_layer2 = dict(model.named_parameters())[\"encoder.block.0.layer.0.SelfAttention.k.weight\"]\n",
        "specific_layer3 = dict(model.named_parameters())[\"encoder.block.0.layer.0.SelfAttention.v.weight\"]\n",
        "specific_layer4 = dict(model.named_parameters())[\"encoder.block.0.layer.0.SelfAttention.o.weight\"]\n",
        "specific_layer5 = dict(model.named_parameters())[\"encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\"]\n",
        "weights_numpy1 = specific_layer1.detach().cpu().numpy()\n",
        "weights_numpy2 = specific_layer2.detach().cpu().numpy()\n",
        "weights_numpy3 = specific_layer3.detach().cpu().numpy()\n",
        "weights_numpy4 = specific_layer4.detach().cpu().numpy()\n",
        "weights_numpy5 = specific_layer5.detach().cpu().numpy()\n",
        "\n",
        "print(weights_numpy1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c34ec099-33ae-424c-8991-ddfb6892fa18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c34ec099-33ae-424c-8991-ddfb6892fa18",
        "outputId": "1f2be233-61ad-4451-ae64-9eee941cf9f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.08417013,  0.02327163,  0.03324942, ...,  0.0095912 ,\n",
              "        -0.00897334,  0.06847523],\n",
              "       [-0.01528336,  0.03679903,  0.01168111, ..., -0.08828606,\n",
              "        -0.06326146, -0.08330554],\n",
              "       [ 0.0158311 , -0.01872572, -0.01461039, ...,  0.01536664,\n",
              "        -0.00671326, -0.152606  ],\n",
              "       ...,\n",
              "       [ 0.06067085, -0.07317689,  0.02802195, ..., -0.01628867,\n",
              "        -0.03041755, -0.04879983],\n",
              "       [-0.03634617,  0.02373624, -0.05132531, ...,  0.02156651,\n",
              "         0.04427908, -0.07474238],\n",
              "       [-0.14625014, -0.05933198,  0.06540201, ..., -0.03239545,\n",
              "         0.06999768, -0.03140048]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "weights_numpy1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee45523a-5400-4c00-a34b-d21d86031fb7",
      "metadata": {
        "id": "ee45523a-5400-4c00-a34b-d21d86031fb7"
      },
      "source": [
        "### Modify the Weights\n",
        "If you want to perform modifications (e.g., **for an adversarial attack or fine-tuning**), you can do so directly in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c6cbdfe5-0b9f-45cb-a7cb-f7ac584193bc",
      "metadata": {
        "id": "c6cbdfe5-0b9f-45cb-a7cb-f7ac584193bc"
      },
      "outputs": [],
      "source": [
        "# # Modify weights in-place\n",
        "# import torch\n",
        "# with torch.no_grad():\n",
        "#     specific_layer1 += 1.0  # Add noise or perturbation\n",
        "# with torch.no_grad():\n",
        "#     specific_layer2 += 1.0  # Add noise or perturbation\n",
        "# with torch.no_grad():\n",
        "#     specific_layer3 += 1.0  # Add noise or perturbation\n",
        "# with torch.no_grad():\n",
        "#     specific_layer4 += 1.0  # Add noise or perturbation\n",
        "# with torch.no_grad():\n",
        "#     specific_layer5 += 1.0  # Add noise or perturbation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c5d946-8ccf-4c99-930d-a776683c40e0",
      "metadata": {
        "id": "51c5d946-8ccf-4c99-930d-a776683c40e0"
      },
      "source": [
        "After modifying the weights, you can save the model back to a directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6067d9ff-c483-4f3d-a6e7-6ba992d94eb6",
      "metadata": {
        "id": "6067d9ff-c483-4f3d-a6e7-6ba992d94eb6"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained(\"./modified_model\")\n",
        "# tokenizer.save_pretrained(\"./modified_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "110c76e9-2db1-403e-a4bf-d6f78f2326c9",
      "metadata": {
        "id": "110c76e9-2db1-403e-a4bf-d6f78f2326c9",
        "outputId": "da176084-bd48-4a20-b928-c3e3e6d7bfb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'I would like to marry you and live my life with you.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pten_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "pten_pipeline(\"translate Portuguese to English: Eu gostaria de me casar com você e viver minha vida com você.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8c2da107-58ac-432b-8cc3-270dfb363044",
      "metadata": {
        "id": "8c2da107-58ac-432b-8cc3-270dfb363044",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e186ce7-4b46-4d5a-aca8-a257a5612310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Translation: I would like to marry you and live my life with you.\n",
            "Adversarial Translation: I would like to get married with you and live my \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Detect device\n",
        "\n",
        "model.to(device)  # Move model to the selected device\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Input text in Portuguese\n",
        "input_text = \"Eu gostaria de me casar com você e viver minha vida com você.\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Move input tensors to device\n",
        "\n",
        "# Target text (optional, for supervised loss calculation)\n",
        "target_text = \"This is an example of an attack.\"\n",
        "target_ids = tokenizer(target_text, return_tensors=\"pt\").input_ids.to(device)  # Move target tensors to device\n",
        "\n",
        "# Get input embeddings with gradient tracking\n",
        "input_ids = inputs.input_ids\n",
        "embeddings = model.encoder.embed_tokens(input_ids).to(device)  # Move embeddings to device\n",
        "embeddings = embeddings.clone().detach().requires_grad_(True)  # Enable gradient tracking\n",
        "\n",
        "# Forward pass using embeddings\n",
        "outputs = model(labels=target_ids, inputs_embeds=embeddings)\n",
        "loss = outputs.loss\n",
        "\n",
        "# Compute gradients\n",
        "loss.backward()  # Backpropagate\n",
        "\n",
        "# Apply FGSM perturbation\n",
        "epsilon = 5.0  # Small perturbation size\n",
        "adversarial_embeds = embeddings + epsilon * embeddings.grad.sign()\n",
        "\n",
        "# Generate adversarial translation\n",
        "with torch.no_grad():\n",
        "    adversarial_outputs = model.generate(inputs_embeds=adversarial_embeds)\n",
        "    adversarial_translation = tokenizer.decode(adversarial_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Display results\n",
        "original_translation = tokenizer.decode(model.generate(**inputs)[0], skip_special_tokens=True)\n",
        "print(\"Original Translation:\", original_translation)\n",
        "print(\"Adversarial Translation:\", adversarial_translation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a73160a-a2e9-47bd-8499-bb477fdebd2d",
      "metadata": {
        "id": "1a73160a-a2e9-47bd-8499-bb477fdebd2d"
      },
      "source": [
        "As we can notice, the gradient magnitude is very small. This is the reason why the perturbation is not affecting the output for values of epsilon close to 0.\n",
        "Performing a embedding-level perturbation, we need at least a epsilon >= 5."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a6f0eb5-0780-4ab4-b7e2-f3d9951329b0",
      "metadata": {
        "id": "3a6f0eb5-0780-4ab4-b7e2-f3d9951329b0"
      },
      "source": [
        "### How the embedding process works\n",
        "\n",
        "The BERT model first splits the sentence into tokens using a tokenizer trained on the BPE algorithm. Then, in the first layer, which is an embedding layer, each ID is converted into a vector.\n",
        "\n",
        "Inside BERT, as well as most other NLP deep learning models, the conversion from token IDs to vectors is done with an Embedding layer. For instance, in Pytorch it is the torch.nn.Embedding module.\n",
        "\n",
        "The embedding layer works as a lookup table: it contains a table of vectors so that indexing such a table with a token index gives you the token vector. The embedding layer receives a tensor with integer values and outputs a tensor with the vectors associated with the input's token indices.\n",
        "\n",
        "The lookup table inside the embedding layer is updated like any other trainable parameter in the model during the optimization steps.\n",
        "\n",
        "The embedding layer is not a separate thing, but an integral part of the model.\n",
        "\n",
        "Note that the embedding layer is coupled to the tokenizer used to convert the input text into token indices, because the token indices generated by the tokenizer are used as indices to the embedding layer's lookup table, so they must match."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b4d5612-79f3-4abc-bdb4-895ab59a2774",
      "metadata": {
        "id": "7b4d5612-79f3-4abc-bdb4-895ab59a2774"
      },
      "source": [
        "Now we should apply the perturbation on the test set and see what is the reasult in terms of accuracy. From the file ParaCrawl99K we can take 99k examples of pairs of sentences with the related Google Translator translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dd86ea33-fb50-4284-ba6a-b2974a81b1a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd86ea33-fb50-4284-ba6a-b2974a81b1a0",
        "outputId": "ff6c6d26-6c50-4dbe-d9ae-e419f2ff4a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper's autors created a general-domain test set from the\n",
        "ParaCrawl dataset. We begin by randomly selecting 128,000 sentence pairs from its 20M pairs.\n",
        "ParaCrawl is originally deduplicated, but similar\n",
        "sentences still might exist in our split of the training\n",
        "and test sets. Thus, they apply as stricter deduplication process to increase the quality of our test\n",
        "set. All sentences with similarity greater than\n",
        "a threshold were discarded from the test set. LSH found\n",
        "28,913 sentences in the test set with a similarity\n",
        "score above 0.7 of sentences in the training set.\n",
        "The final test set ended up having 99,087 sentence\n",
        "pairs, which we called ParaCrawl 99k test set. This\n",
        "dataset and its corresponding translations using GT\n",
        "are available in their GitHub repository."
      ],
      "metadata": {
        "id": "tZ4xT6DB9-3g"
      },
      "id": "tZ4xT6DB9-3g"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2221ddbe-0b34-4820-b129-63c28251b8d6",
      "metadata": {
        "id": "2221ddbe-0b34-4820-b129-63c28251b8d6"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# import torch\n",
        "# from nltk.translate.bleu_score import corpus_bleu\n",
        "# from sacrebleu import corpus_bleu  # For BLEU score calculation\n",
        "# import random\n",
        "\n",
        "# # Check if GPU is available\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# model.eval()  # Set model to evaluation mode\n",
        "# model.to(device)\n",
        "\n",
        "# # Load the test set from the pkl file\n",
        "# with open(\"ParaCrawl99K_PtEn_PCrawlGoogleT.pkl\", \"rb\") as file:\n",
        "#     test_set = pickle.load(file)  # Expecting a list of (input_sentence, target_translation)\n",
        "\n",
        "# # Shuffle the dataset\n",
        "# random.shuffle(test_set)\n",
        "\n",
        "# # Take a random sample of 100 elements\n",
        "# test_set = test_set[:100]\n",
        "\n",
        "# # Parameters\n",
        "# epsilon = 0.5  # Perturbation size\n",
        "# original_translations = []\n",
        "# adversarial_translations = []\n",
        "# target_translations = []\n",
        "\n",
        "# # Process the dataset\n",
        "# for input_text, target_text in test_set:\n",
        "#     # Tokenize input and target\n",
        "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "#     target_ids = tokenizer(target_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "#     # Get input embeddings with gradient tracking\n",
        "#     input_ids = inputs.input_ids.to(device)\n",
        "#     embeddings = model.encoder.embed_tokens(input_ids).to(device)\n",
        "#     embeddings = embeddings.clone().detach().requires_grad_(True).to(device)  # Enable gradient tracking\n",
        "\n",
        "#     # Forward pass using embeddings\n",
        "#     outputs = model(labels=target_ids, inputs_embeds=embeddings)\n",
        "#     loss = outputs.loss\n",
        "#     loss.backward()  # Compute gradients\n",
        "\n",
        "#     # Apply FGSM perturbation\n",
        "#     adversarial_embeds = embeddings + epsilon * embeddings.grad.sign()\n",
        "\n",
        "#     # Generate translations\n",
        "#     with torch.no_grad():\n",
        "#         original_output = model.generate(**inputs, num_beams=1)\n",
        "#         adversarial_output = model.generate(inputs_embeds=adversarial_embeds, num_beams=1)\n",
        "\n",
        "#     # Decode translations\n",
        "#     original_translation = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
        "#     adversarial_translation = tokenizer.decode(adversarial_output[0], skip_special_tokens=True)\n",
        "\n",
        "#     # Save results\n",
        "#     original_translations.append(original_translation)\n",
        "#     adversarial_translations.append(adversarial_translation)\n",
        "#     target_translations.append(target_text)\n",
        "\n",
        "# # Evaluate the impact of the adversarial attack\n",
        "# original_bleu = corpus_bleu(original_translations, [target_translations]).score\n",
        "# adversarial_bleu = corpus_bleu(adversarial_translations, [target_translations]).score\n",
        "\n",
        "# # Display results\n",
        "# print(\"Original BLEU Score:\", original_bleu)\n",
        "# print(\"Adversarial BLEU Score:\", adversarial_bleu)\n",
        "\n",
        "# # Save results for later analysis\n",
        "# results = {\n",
        "#     \"original_translations\": original_translations,\n",
        "#     \"adversarial_translations\": adversarial_translations,\n",
        "#     \"target_translations\": target_translations,\n",
        "# }\n",
        "# with open(\"adversarial_results.pkl\", \"wb\") as file:\n",
        "#     pickle.dump(results, file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3588889a-df02-4ede-a5b0-a5164d2313ee",
      "metadata": {
        "id": "3588889a-df02-4ede-a5b0-a5164d2313ee"
      },
      "source": [
        "**BLEU** (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: \"the closer a machine translation is to a professional human translation, the better it is\" – this is the central idea behind BLEU.\n",
        "\n",
        "Scores are calculated for individual translated segments, generally sentences, by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality.\n",
        "\n",
        "BLEU's output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts. Few human translations will attain a score of 1, since this would indicate that the candidate is identical to one of the reference translations. For this reason, it is not necessary to attain a score of 1. Because there are more opportunities to match, adding additional reference translations will increase the BLEU score (put in the dataset more than 1 correct translation).\n",
        "\n",
        "For the English-Portuguese case they obtain a BLEU=31."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0121f8a3-1644-4631-94b0-4cc49bea32bd",
      "metadata": {
        "id": "0121f8a3-1644-4631-94b0-4cc49bea32bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddb76ed-15e5-4715-f812-e254e4505d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Original BLEU Score: 53.48259312838876\n",
            "Adversarial BLEU Score: 42.7287006396234\n",
            "Defended BLEU Score: 13.134549472120794\n",
            "epsilon: 3.0\n",
            "lambdareg: 15.0\n",
            "noisestd: 0.3\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sacrebleu import corpus_bleu  # For BLEU score calculation\n",
        "import random\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model.eval()  # Set model to evaluation mode\n",
        "model.to(device)\n",
        "\n",
        "# Load the test set from the pkl file\n",
        "with open(\"ParaCrawl99K_PtEn_PCrawlGoogleT.pkl\", \"rb\") as file:\n",
        "    test_set = pickle.load(file)  # Expecting a list of (input_sentence, target_translation)\n",
        "\n",
        "# Shuffle the dataset\n",
        "#random.shuffle(test_set)\n",
        "\n",
        "# Take a random sample of 100 elements\n",
        "test_set = test_set[:50]\n",
        "\n",
        "# Parameters\n",
        "epsilon = 3.0  # Perturbation size for FGSM\n",
        "lambda_reg = 15.0  # Gradient regularization strength\n",
        "noise_std = 0.3  # Standard deviation for input noise\n",
        "original_translations = []\n",
        "adversarial_translations = []\n",
        "defended_translations = []\n",
        "target_translations = []\n",
        "\n",
        "\n",
        "# FGSM Attack Function\n",
        "def fgsm_attack(embeddings, gradients, epsilon):\n",
        "    return embeddings + epsilon * gradients.sign()\n",
        "\n",
        "\n",
        "# Defense Function: Gradient Regularization + Input Noise\n",
        "def defend_with_regularization_and_noise(model, tokenizer, adversarial_embeddings, lambda_reg, noise_std):\n",
        "    # Ensure embeddings require gradients\n",
        "    adversarial_embeddings = adversarial_embeddings.clone().detach().requires_grad_(True).to(device)\n",
        "\n",
        "    # Forward pass with adversarial embeddings\n",
        "    outputs = model(inputs_embeds=adversarial_embeddings, labels=target_ids)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "    # Compute gradient regularization term\n",
        "    grad_norm = torch.norm(adversarial_embeddings.grad, p=2)\n",
        "    reg_term = lambda_reg * grad_norm\n",
        "\n",
        "    # Add Gaussian noise to embeddings as part of the defense\n",
        "    noisy_embeddings = adversarial_embeddings + torch.normal(mean=0, std=noise_std, size=adversarial_embeddings.size()).to(device)\n",
        "\n",
        "    # Combine noise and gradient regularization\n",
        "    defended_embeddings = noisy_embeddings - lambda_reg * adversarial_embeddings.grad.sign()\n",
        "\n",
        "    # Generate translation with defended embeddings\n",
        "    with torch.no_grad():\n",
        "        defended_output = model.generate(inputs_embeds=defended_embeddings, num_beams=1)\n",
        "        defended_translation = tokenizer.decode(defended_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Clear gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    return defended_translation\n",
        "\n",
        "\n",
        "\n",
        "# Main Loop\n",
        "for input_text, target_text in test_set:\n",
        "    # Tokenize input and target\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    target_ids = tokenizer(target_text, return_tensors=\"pt\", truncation=True).input_ids.to(device)\n",
        "\n",
        "    # Get embeddings with gradient tracking\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    embeddings = model.encoder.embed_tokens(input_ids).clone().detach().requires_grad_(True).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs_embeds=embeddings, labels=target_ids)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()  # Compute gradients\n",
        "\n",
        "    # FGSM Attack\n",
        "    adversarial_embeddings = fgsm_attack(embeddings, embeddings.grad, epsilon)\n",
        "\n",
        "    # Generate adversarial translation\n",
        "    with torch.no_grad():\n",
        "        adversarial_output = model.generate(inputs_embeds=adversarial_embeddings, num_beams=1)\n",
        "        adversarial_translation = tokenizer.decode(adversarial_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Defense\n",
        "    defended_translation = defend_with_regularization_and_noise(model, tokenizer, adversarial_embeddings, lambda_reg, noise_std)\n",
        "\n",
        "\n",
        "    # Generate original translation\n",
        "    with torch.no_grad():\n",
        "        original_output = model.generate(inputs_embeds=embeddings, num_beams=1)\n",
        "        original_translation = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Save results\n",
        "    original_translations.append(original_translation)\n",
        "    adversarial_translations.append(adversarial_translation)\n",
        "    defended_translations.append(defended_translation)\n",
        "    target_translations.append([target_text])\n",
        "\n",
        "# Evaluate BLEU scores\n",
        "original_bleu = corpus_bleu(original_translations, target_translations).score\n",
        "adversarial_bleu = corpus_bleu(adversarial_translations, target_translations).score\n",
        "defended_bleu = corpus_bleu(defended_translations, target_translations).score\n",
        "\n",
        "# Display results\n",
        "print(\"Original BLEU Score:\", original_bleu)\n",
        "print(\"Adversarial BLEU Score:\", adversarial_bleu)\n",
        "print(\"Defended BLEU Score:\", defended_bleu)\n",
        "print(\"epsilon:\", epsilon)\n",
        "print(\"lambdareg:\", lambda_reg)\n",
        "print(\"noisestd:\", noise_std)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}